The primary mechanism for limiting the amount of data the server can determine regarding message arrive metadata is a combination of periodically seeding the mailbox with fake messages, and scheduled message mixes.

\includegraphics[width=0.5\textwidth]{scheduler}

In the diagram above, a mix occurs at every event M. We 







a combination of scheduled exchanges, mixing, and fake messages





a separate scheduler module that releases UP, DOWN, or NONE tick every time t
we feed it information about how many messages we're actually receiving, and it lets us know what the size of our mailbox should be, leaking only 1 trit of information to the server every time t.
up : increase size of mailbox
down: decrease
none: same


option 1: just have many extra pushes


option 2: mixing, mailbox grows with messages received
since we're always pushing on schedule, either deletes will leak which are fake, or we have to wait a long time before we catch a fake
deletes won't leak which are fake if we have a large enough mix number, but that's infeasible.


option 3: mixing, but there's a good amount of decoys. half would be super nice but we don't need that.
conceptualize as two separate operations

each mix, if we pulled down a fake, we can push up a real
since we have many fakes, we have a pretty good chance of getting a fake so we can push up a real
then we're not bayes theoreming/ monty halling it, there's no additional probability of a fake being pulled down and you can't at all tell the difference between a regular mix and a real-insert mix

separately, we add fake messages in to replenish the fake messages relative to the size of the inbox. the server will know that 1 in mix number + 1 of these are fake, but they have just as high probability of being selected later. and if we have enough, then even if it's selected to come down, there won't necessarily be a real one in the replacements. ***do the numbers here.

if we delete, we know that one of those mix num was fake, but we've been mixing a bunch so that should be fine. (as long as it's deleted after this many mixes, which works out to this much time)




The scheduler considers how often messages are being received and how often scheduled communications happen, and returns a result to keep overall server growth consistent with actual message receipt and deletion rates.



To enable deletion, we must include 



Even if the user never deletes a message, the scheduler should still periodically return a DOWN bit, so that the contents of the server do not needlessly continuously grow at a high rate. In the case where deletes never occur, one could imagine a schedule of only pushes, timed such that is it similar to the actual receipt rate, say every 20 minutes. This is an unacceptable rate of checking for mail in a modern system, where periodic updates occur an order of magnitude faster. Additionally, since only one message is pushed every $t$ minutes, messages received in a batch of $k$ would be delayed at least $k*t$ minutes.

% Learning Update Schedules - Exponential backoff?

\label{mixing}
\subsection{Message Mixing}
The scheduling mechanism is necessary but not sufficient to hide message arrival metadata from the server.




Is this feasible? Certainly with no space constraints, it is, but we must also consider this solution under reasonable constraints. Gmail, a popular email provider, allots 17 GB of space to a user. Assume that a user wants to use this email address for 5 years (a conservative estimate), and that messages average 170 kB (this number is accurate for my personal email account). Then, 100,000 messages will fit into the server. Say that we are comfortable with having 32 fake messages uploaded per delete operation. Even discounting other messages received in the mail server, this only allows 3125 delete operations, which amounts to 1.7 delete operations per day over the five years. This amount to a delay of about 7 hours any time the user would like to delete a message \--- and this is not taking into account the space that real messages will use. While a user without space constraints could certainly choose this method, we will now describe a more feasible solution.